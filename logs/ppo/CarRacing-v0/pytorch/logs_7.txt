Model: <class 'models.singleagent.ppo.PPOAgent'>, Env: CarRacing-v0/pytorch, Date: 25/03/2020 09:48:53
CPU: 20 Core, 0.0GHz, 377.59 GB, Linux-4.14.167-llgrid-10ms-x86_64-with-debian-buster-sid
GPU 0: Tesla V100-PCIE-32GB, 32.51 GB (Driver: 440.33.01)
Git URL: git@github.com:shawnmanuel000/WorldModelsForDeepRL.git
Hash: ad6d58e8d51acff8f14ae12452052440a4ed209c
Branch: master

num_envs: 16,
state_size: (96, 96, 3),
action_size: (3,),
action_space: Box(3,),
envs: <class 'utils.envs.EnvManager'>,
statemodel: <utils.wrappers.ImgStack object at 0x7f18e404cc18>,

import torch
import numpy as np
from utils.rand import ReplayBuffer, PrioritizedReplayBuffer
from utils.network import PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE, DISCOUNT_RATE, NUM_STEPS, one_hot_from_indices

BATCH_SIZE = 32					# Number of samples to train on for each train step
PPO_EPOCHS = 2					# Number of iterations to sample batches for training
ENTROPY_WEIGHT = 0.005			# The weight for the entropy term of the Actor loss
CLIP_PARAM = 0.1				# The limit of the ratio of new action probabilities to old probabilities

class PPOActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.discrete = type(action_size) != tuple
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, action_size[-1])
		self.action_sig = torch.nn.Parameter(torch.zeros(action_size[-1]))
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
		self.dist = lambda m,s: torch.distributions.Categorical(m.softmax(-1)) if self.discrete else torch.distributions.Normal(m,s)
		
	def forward(self, state, action_in=None, sample=True):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		state = self.layer3(state).relu()
		action_mu = self.action_mu(state)
		action_sig = self.action_sig.exp().expand_as(action_mu)
		dist = self.dist(action_mu, action_sig)
		action = dist.sample() if action_in is None else action_in.argmax(-1) if self.discrete else action_in
		action_out = one_hot_from_indices(action, action_mu.size(-1)) if self.discrete else action
		log_prob = dist.log_prob(action)
		entropy = dist.entropy()
		return action_out, log_prob, entropy

class PPOCritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)!=3 else Conv(state_size, INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.value = torch.nn.Linear(CRITIC_HIDDEN, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		state = self.layer3(state).relu()
		value = self.value(state)
		return value

class PPONetwork(PTACNetwork):
	def __init__(self, state_size, action_size, actor=PPOActor, critic=PPOCritic, lr=LEARN_RATE, tau=None, gpu=True, load=None, name="ppo"):
		super().__init__(state_size, action_size, actor=actor, critic=critic, lr=lr, gpu=gpu, load=load, name=name)

	def get_action_probs(self, state, action_in=None, grad=False, numpy=False, sample=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action, log_prob, entropy = self.actor_local(state.to(self.device), action_in, sample)
			action_or_entropy = action if action_in is None else entropy.mean()
			return (x.cpu().numpy() if numpy else x for x in [action_or_entropy, log_prob])

	def get_value(self, state, grad=False, numpy=False):
		with torch.enable_grad() if grad else torch.no_grad():
			return self.critic_local(state.to(self.device)).cpu().numpy() if numpy else self.critic_local(state.to(self.device))

	def optimize(self, states, actions, old_log_probs, targets, advantages, clip_param=CLIP_PARAM, e_weight=ENTROPY_WEIGHT, scale=1):
		values = self.get_value(states, grad=True)
		critic_loss = (values - targets).pow(2) * scale
		self.step(self.critic_optimizer, critic_loss.mean())

		entropy, new_log_probs = self.get_action_probs(states, actions, grad=True)
		ratio = (new_log_probs - old_log_probs).exp()
		ratio_clipped = torch.clamp(ratio, 1.0-clip_param, 1.0+clip_param)
		actor_loss = -(torch.min(ratio*advantages, ratio_clipped*advantages) + e_weight*entropy) * scale
		self.step(self.actor_optimizer, actor_loss.mean())

class PPOAgent(PTACAgent):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, update_freq=NUM_STEPS, gpu=True, load=None):
		super().__init__(state_size, action_size, PPONetwork, lr=lr, update_freq=update_freq, gpu=gpu, load=load)

	def get_action(self, state, eps=None, sample=True):
		self.action, self.log_prob = self.network.get_action_probs(self.to_tensor(state), numpy=True, sample=sample)
		return np.tanh(self.action)

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, self.action, self.log_prob, reward, done))
		if np.any(done[0]) or len(self.buffer) >= self.update_freq:
			states, actions, log_probs, rewards, dones = map(self.to_tensor, zip(*self.buffer))
			self.buffer.clear()
			states = torch.cat([states, self.to_tensor(next_state).unsqueeze(0)], dim=0)
			values = self.network.get_value(states)
			targets, advantages = self.compute_gae(values[-1], rewards.unsqueeze(-1), dones.unsqueeze(-1), values[:-1], gamma=DISCOUNT_RATE)
			states, actions, log_probs, targets, advantages = [x.view(x.size(0)*x.size(1), *x.size()[2:]) for x in (states[:-1], actions, log_probs, targets, advantages)]
			self.replay_buffer.clear().extend(list(zip(states, actions, log_probs, targets, advantages)), shuffle=True)
			for _ in range((len(self.replay_buffer)*PPO_EPOCHS)//BATCH_SIZE):
				state, action, log_prob, target, advantage = self.replay_buffer.next_batch(BATCH_SIZE, torch.stack)
				self.network.optimize(state, action, log_prob, target, advantage)
				
REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.0004   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 512				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.99			# The discount rate to use in the Bellman Equation
NUM_STEPS = 500					# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.020               	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.980             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
REPLAY_BATCH_SIZE = 32        	# How many experience tuples to sample from the buffer for each train step
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer
SAVE_DIR = "./saved_models"

import os
import gym
import torch
import argparse
import numpy as np
from envs import make_env, all_envs, env_name
from models import all_models, EPS_MIN
from utils.rand import RandomAgent
from utils.misc import Logger, rollout
from utils.envs import EnsembleEnv, EnvManager, EnvWorker
from utils.wrappers import WorldACAgent
from utils.multiprocess import set_rank_size

TRIAL_AT = 1000
SAVE_AT = 1

def train(make_env, model, ports, steps, checkpoint=None, save_best=True, log=True, render=False, worldmodel=True):
	envs = (EnvManager if len(ports)>0 else EnsembleEnv)(make_env, ports if ports else 4)
	agent = WorldACAgent(envs.state_size, envs.action_size, model, envs.num_envs, load=checkpoint, gpu=True, worldmodel=worldmodel) 
	logger = Logger(model, checkpoint, num_envs=envs.num_envs, state_size=agent.state_size, action_size=envs.action_size, action_space=envs.env.action_space, envs=type(envs), statemodel=agent.state_model)
	states = envs.reset(train=True)
	total_rewards = []
	for s in range(steps+1):
		env_actions, actions, states = agent.get_env_action(envs.env, states)
		next_states, rewards, dones, _ = envs.step(env_actions, train=True)
		agent.train(states, actions, next_states, rewards, dones)
		states = next_states
		if s%TRIAL_AT==0:
			rollouts = rollout(envs, agent, render=render)
			total_rewards.append(np.round(np.mean(rollouts, axis=-1), 3))
			if checkpoint and len(total_rewards)%SAVE_AT==0: agent.save_model(checkpoint)
			if checkpoint and save_best and np.all(total_rewards[-1] >= np.max(total_rewards, axis=-1)): agent.save_model(checkpoint, "best")
			if log: logger.log(f"Step: {s:7d}, Reward: {total_rewards[-1]} [{np.std(rollouts):4.3f}], Avg: {round(np.mean(total_rewards, axis=0),3)} ({agent.acagent.eps:.4f})")
	envs.close()

def trial(make_env, model, checkpoint=None, render=False):
	envs = EnsembleEnv(make_env, 1)
	agent = WorldACAgent(envs.state_size, envs.action_size, model, envs.num_envs, load=checkpoint, train=False, gpu=False, worldmodel=True)
	print(f"Reward: {rollout(envs, agent, eps=EPS_MIN, render=render)}")
	envs.close()

def parse_args(all_envs, all_models):
	parser = argparse.ArgumentParser(description="A3C Trainer")
	parser.add_argument("--env_name", type=str, default=env_name, choices=all_envs, help="Name of the environment to use. Allowed values are:\n"+', '.join(all_envs), metavar="env_name")
	parser.add_argument("--model", type=str, default="ppo", choices=all_models, help="Which RL algorithm to use. Allowed values are:\n"+', '.join(all_models), metavar="model")
	parser.add_argument("--iternum", type=int, default=-1, choices=[-1,0,1], help="Whether to train using World Model to load (0 or 1) or raw images (-1)")
	parser.add_argument("--tcp_ports", type=int, default=[], nargs="+", help="The list of worker ports to connect to")
	parser.add_argument("--tcp_rank", type=int, default=0, help="Which port to listen on (as a worker server)")
	parser.add_argument("--render", action="store_true", help="Whether to render an environment rollout")
	parser.add_argument("--trial", action="store_true", help="Whether to show a trial run training on the Pendulum-v0 environment")
	parser.add_argument("--steps", type=int, default=100000, help="Number of steps to train the agent")
	args = parser.parse_args()
	return args

if __name__ == "__main__":
	args = parse_args(all_envs, all_models.keys())
	checkpoint = f"{args.env_name}/pytorch" if args.iternum < 0 else f"{args.env_name}/iter{args.iternum}"
	rank, size = set_rank_size(args.tcp_rank, args.tcp_ports)
	get_env = lambda: make_env(args.env_name, args.render)
	model = all_models[args.model]
	if rank>0:
		EnvWorker(make_env=get_env).start()
	elif args.trial:
		trial(make_env=get_env, model=model, checkpoint=checkpoint, render=args.render)
	else:
		train(make_env=get_env, model=model, ports=list(range(1,size)), steps=args.steps, checkpoint=checkpoint, render=args.render, worldmodel=args.iternum>=0)


Step:       0, Reward: -32.456 [5.713], Avg: -32.456 (1.0000) <0-00:00:00> 
Step:    1000, Reward: -29.379 [5.280], Avg: -30.918 (1.0000) <0-00:00:48> 
Step:    2000, Reward: -29.26 [5.479], Avg: -30.365 (1.0000) <0-00:01:38> 
Step:    3000, Reward: -26.354 [9.898], Avg: -29.362 (1.0000) <0-00:02:58> 
Step:    4000, Reward: -27.721 [11.375], Avg: -29.034 (1.0000) <0-00:04:18> 
Step:    5000, Reward: -30.226 [10.495], Avg: -29.233 (1.0000) <0-00:05:38> 
Step:    6000, Reward: -35.931 [9.418], Avg: -30.19 (1.0000) <0-00:06:58> 
Step:    7000, Reward: -49.569 [7.388], Avg: -32.612 (1.0000) <0-00:08:19> 
Step:    8000, Reward: -43.224 [11.077], Avg: -33.791 (1.0000) <0-00:09:40> 
Step:    9000, Reward: -51.474 [8.588], Avg: -35.559 (1.0000) <0-00:11:00> 
Step:   10000, Reward: -50.113 [8.257], Avg: -36.882 (1.0000) <0-00:12:21> 
Step:   11000, Reward: -45.494 [11.756], Avg: -37.6 (1.0000) <0-00:13:42> 
Step:   12000, Reward: -47.769 [8.159], Avg: -38.382 (1.0000) <0-00:15:04> 
Step:   13000, Reward: -32.21 [14.350], Avg: -37.941 (1.0000) <0-00:16:24> 
Step:   14000, Reward: -36.629 [9.424], Avg: -37.854 (1.0000) <0-00:17:43> 
Step:   15000, Reward: -19.31 [26.229], Avg: -36.695 (1.0000) <0-00:19:03> 
Step:   16000, Reward: -34.536 [12.452], Avg: -36.568 (1.0000) <0-00:20:25> 
Step:   17000, Reward: -39.487 [15.608], Avg: -36.73 (1.0000) <0-00:21:46> 
Step:   18000, Reward: -30.09 [24.294], Avg: -36.381 (1.0000) <0-00:23:08> 
Step:   19000, Reward: -30.467 [26.285], Avg: -36.085 (1.0000) <0-00:24:29> 
Step:   20000, Reward: -39.748 [10.836], Avg: -36.259 (1.0000) <0-00:25:49> 
Step:   21000, Reward: -34.558 [22.506], Avg: -36.182 (1.0000) <0-00:27:09> 
Step:   22000, Reward: -40.389 [16.793], Avg: -36.365 (1.0000) <0-00:28:29> 
Step:   23000, Reward: -44.744 [35.410], Avg: -36.714 (1.0000) <0-00:29:49> 
Step:   24000, Reward: -36.219 [20.940], Avg: -36.694 (1.0000) <0-00:31:10> 
Step:   25000, Reward: -17.61 [28.202], Avg: -35.96 (1.0000) <0-00:32:30> 
Step:   26000, Reward: -9.997 [31.910], Avg: -34.999 (1.0000) <0-00:33:50> 
Step:   27000, Reward: -18.469 [32.167], Avg: -34.408 (1.0000) <0-00:35:11> 
Step:   28000, Reward: -22.42 [22.865], Avg: -33.995 (1.0000) <0-00:36:32> 
Step:   29000, Reward: 12.651 [50.346], Avg: -32.44 (1.0000) <0-00:37:53> 
Step:   30000, Reward: 24.711 [59.585], Avg: -30.596 (1.0000) <0-00:39:13> 
Step:   31000, Reward: 22.92 [77.636], Avg: -28.924 (1.0000) <0-00:40:34> 
Step:   32000, Reward: 41.816 [94.626], Avg: -26.78 (1.0000) <0-00:41:53> 
Step:   33000, Reward: 68.536 [109.634], Avg: -23.977 (1.0000) <0-00:43:12> 
Step:   34000, Reward: 81.155 [105.236], Avg: -20.973 (1.0000) <0-00:44:30> 
Step:   35000, Reward: 55.657 [130.202], Avg: -18.845 (1.0000) <0-00:45:50> 
Step:   36000, Reward: 173.966 [130.231], Avg: -13.634 (1.0000) <0-00:47:10> 
Step:   37000, Reward: 317.397 [145.952], Avg: -4.922 (1.0000) <0-00:48:30> 
Step:   38000, Reward: 233.854 [175.840], Avg: 1.2 (1.0000) <0-00:49:51> 
Step:   39000, Reward: 338.371 [180.252], Avg: 9.63 (1.0000) <0-00:51:11> 
Step:   40000, Reward: 361.74 [182.535], Avg: 18.218 (1.0000) <0-00:52:31> 
Step:   41000, Reward: 465.267 [179.125], Avg: 28.862 (1.0000) <0-00:53:51> 
Step:   42000, Reward: 395.958 [266.115], Avg: 37.399 (1.0000) <0-00:55:11> 
Step:   43000, Reward: 455.071 [302.373], Avg: 46.891 (1.0000) <0-00:56:31> 
Step:   44000, Reward: 405.066 [287.968], Avg: 54.851 (1.0000) <0-00:57:52> 
Step:   45000, Reward: 590.883 [300.555], Avg: 66.504 (1.0000) <0-00:59:12> 
Step:   46000, Reward: 737.27 [213.773], Avg: 80.775 (1.0000) <0-01:00:34> 
Step:   47000, Reward: 715.988 [263.424], Avg: 94.009 (1.0000) <0-01:01:55> 
Step:   48000, Reward: 783.241 [200.990], Avg: 108.075 (1.0000) <0-01:03:15> 
Step:   49000, Reward: 869.449 [63.391], Avg: 123.302 (1.0000) <0-01:04:35> 
Step:   50000, Reward: 732.179 [170.082], Avg: 135.241 (1.0000) <0-01:05:56> 
Step:   51000, Reward: 548.498 [233.330], Avg: 143.188 (1.0000) <0-01:07:16> 
Step:   52000, Reward: 680.369 [266.575], Avg: 153.324 (1.0000) <0-01:08:36> 
Step:   53000, Reward: 549.974 [260.505], Avg: 160.669 (1.0000) <0-01:09:58> 
Step:   54000, Reward: 760.978 [115.384], Avg: 171.584 (1.0000) <0-01:11:20> 
Step:   55000, Reward: 766.668 [206.550], Avg: 182.21 (1.0000) <0-01:12:43> 
Step:   56000, Reward: 727.509 [160.318], Avg: 191.777 (1.0000) <0-01:14:04> 
Step:   57000, Reward: 672.997 [240.883], Avg: 200.074 (1.0000) <0-01:15:22> 
Step:   58000, Reward: 748.688 [173.756], Avg: 209.372 (1.0000) <0-01:16:46> 
Step:   59000, Reward: 735.384 [135.398], Avg: 218.139 (1.0000) <0-01:18:07> 
Step:   60000, Reward: 832.141 [84.685], Avg: 228.205 (1.0000) <0-01:19:28> 
Step:   61000, Reward: 618.279 [216.958], Avg: 234.496 (1.0000) <0-01:20:49> 
Step:   62000, Reward: 698.735 [178.816], Avg: 241.865 (1.0000) <0-01:22:09> 
Step:   63000, Reward: 480.432 [174.801], Avg: 245.593 (1.0000) <0-01:23:30> 
Step:   64000, Reward: 651.249 [242.588], Avg: 251.834 (1.0000) <0-01:24:50> 
Step:   65000, Reward: 665.588 [220.042], Avg: 258.103 (1.0000) <0-01:26:11> 
Step:   66000, Reward: 647.932 [225.084], Avg: 263.921 (1.0000) <0-01:27:31> 
Step:   67000, Reward: 811.943 [122.928], Avg: 271.98 (1.0000) <0-01:28:53> 
Step:   68000, Reward: 834.833 [84.057], Avg: 280.138 (1.0000) <0-01:30:13> 
Step:   69000, Reward: 748.186 [159.208], Avg: 286.824 (1.0000) <0-01:31:33> 
Step:   70000, Reward: 790.374 [135.627], Avg: 293.916 (1.0000) <0-01:32:53> 
Step:   71000, Reward: 717.233 [262.056], Avg: 299.796 (1.0000) <0-01:34:13> 
Step:   72000, Reward: 773.114 [175.020], Avg: 306.279 (1.0000) <0-01:35:33> 
Step:   73000, Reward: 761.867 [155.274], Avg: 312.436 (1.0000) <0-01:36:54> 
Step:   74000, Reward: 825.0 [96.605], Avg: 319.27 (1.0000) <0-01:38:14> 
Step:   75000, Reward: 725.981 [201.016], Avg: 324.622 (1.0000) <0-01:39:34> 
Step:   76000, Reward: 740.841 [213.119], Avg: 330.027 (1.0000) <0-01:40:55> 
Step:   77000, Reward: 822.435 [133.357], Avg: 336.34 (1.0000) <0-01:42:15> 
Step:   78000, Reward: 798.395 [186.508], Avg: 342.189 (1.0000) <0-01:43:36> 
Step:   79000, Reward: 760.169 [162.177], Avg: 347.414 (1.0000) <0-01:44:56> 
Step:   80000, Reward: 789.84 [167.472], Avg: 352.876 (1.0000) <0-01:46:16> 
Step:   81000, Reward: 775.249 [128.454], Avg: 358.027 (1.0000) <0-01:47:37> 
Step:   82000, Reward: 779.377 [152.926], Avg: 363.103 (1.0000) <0-01:48:56> 
Step:   83000, Reward: 817.747 [78.758], Avg: 368.515 (1.0000) <0-01:50:17> 
Step:   84000, Reward: 835.201 [73.003], Avg: 374.006 (1.0000) <0-01:51:35> 
Step:   85000, Reward: 761.715 [212.806], Avg: 378.514 (1.0000) <0-01:52:56> 
Step:   86000, Reward: 672.157 [245.182], Avg: 381.889 (1.0000) <0-01:54:17> 
Step:   87000, Reward: 640.547 [222.247], Avg: 384.829 (1.0000) <0-01:55:38> 
Step:   88000, Reward: 565.222 [223.550], Avg: 386.856 (1.0000) <0-01:57:00> 
Step:   89000, Reward: 732.014 [143.888], Avg: 390.691 (1.0000) <0-01:58:21> 
Step:   90000, Reward: 753.959 [146.864], Avg: 394.683 (1.0000) <0-01:59:41> 
Step:   91000, Reward: 602.858 [216.454], Avg: 396.945 (1.0000) <0-02:01:02> 
Step:   92000, Reward: 590.554 [253.704], Avg: 399.027 (1.0000) <0-02:02:23> 
Step:   93000, Reward: 655.047 [227.559], Avg: 401.751 (1.0000) <0-02:03:44> 
Step:   94000, Reward: 580.948 [228.010], Avg: 403.637 (1.0000) <0-02:05:05> 
Step:   95000, Reward: 658.995 [220.374], Avg: 406.297 (1.0000) <0-02:06:25> 
Step:   96000, Reward: 621.514 [253.561], Avg: 408.516 (1.0000) <0-02:07:47> 
Step:   97000, Reward: 676.022 [202.148], Avg: 411.245 (1.0000) <0-02:09:08> 
Step:   98000, Reward: 670.799 [157.382], Avg: 413.867 (1.0000) <0-02:10:29> 
Step:   99000, Reward: 555.17 [267.792], Avg: 415.28 (1.0000) <0-02:11:50> 
Step:  100000, Reward: 743.206 [144.790], Avg: 418.527 (1.0000) <0-02:13:11> 
Step:  101000, Reward: 728.682 [206.496], Avg: 421.568 (1.0000) <0-02:14:31> 
Step:  102000, Reward: 685.791 [205.945], Avg: 424.133 (1.0000) <0-02:15:52> 
Step:  103000, Reward: 787.061 [135.827], Avg: 427.623 (1.0000) <0-02:17:14> 
Step:  104000, Reward: 761.336 [151.761], Avg: 430.801 (1.0000) <0-02:18:31> 
Step:  105000, Reward: 773.415 [199.507], Avg: 434.033 (1.0000) <0-02:19:51> 
Step:  106000, Reward: 804.669 [108.477], Avg: 437.497 (1.0000) <0-02:21:17> 
Step:  107000, Reward: 750.788 [190.046], Avg: 440.398 (1.0000) <0-02:22:37> 
Step:  108000, Reward: 682.571 [219.599], Avg: 442.62 (1.0000) <0-02:23:57> 
Step:  109000, Reward: 805.038 [114.788], Avg: 445.914 (1.0000) <0-02:25:17> 
Step:  110000, Reward: 793.41 [171.540], Avg: 449.045 (1.0000) <0-02:26:37> 
Step:  111000, Reward: 781.549 [183.049], Avg: 452.014 (1.0000) <0-02:27:58> 
Step:  112000, Reward: 821.658 [111.969], Avg: 455.285 (1.0000) <0-02:29:21> 
Step:  113000, Reward: 795.06 [209.063], Avg: 458.265 (1.0000) <0-02:30:43> 
Step:  114000, Reward: 849.504 [77.062], Avg: 461.667 (1.0000) <0-02:32:03> 
Step:  115000, Reward: 842.716 [66.981], Avg: 464.952 (1.0000) <0-02:33:24> 
Step:  116000, Reward: 773.616 [162.926], Avg: 467.591 (1.0000) <0-02:34:45> 
Step:  117000, Reward: 750.943 [154.066], Avg: 469.992 (1.0000) <0-02:36:07> 
Step:  118000, Reward: 766.464 [224.692], Avg: 472.483 (1.0000) <0-02:37:29> 
Step:  119000, Reward: 793.741 [150.790], Avg: 475.16 (1.0000) <0-02:38:52> 
Step:  120000, Reward: 847.928 [86.694], Avg: 478.241 (1.0000) <0-02:40:13> 
Step:  121000, Reward: 811.541 [97.560], Avg: 480.973 (1.0000) <0-02:41:34> 
Step:  122000, Reward: 766.883 [156.238], Avg: 483.297 (1.0000) <0-02:42:54> 
Step:  123000, Reward: 838.064 [111.465], Avg: 486.158 (1.0000) <0-02:44:15> 
Step:  124000, Reward: 708.703 [202.676], Avg: 487.939 (1.0000) <0-02:45:36> 
Step:  125000, Reward: 768.926 [148.121], Avg: 490.169 (1.0000) <0-02:46:58> 
Step:  126000, Reward: 801.323 [165.574], Avg: 492.619 (1.0000) <0-02:48:18> 
Step:  127000, Reward: 747.293 [189.544], Avg: 494.609 (1.0000) <0-02:49:40> 
Step:  128000, Reward: 852.698 [81.450], Avg: 497.384 (1.0000) <0-02:51:00> 
Step:  129000, Reward: 824.804 [166.311], Avg: 499.903 (1.0000) <0-02:52:20> 
Step:  130000, Reward: 847.934 [54.555], Avg: 502.56 (1.0000) <0-02:53:41> 
Step:  131000, Reward: 818.022 [143.535], Avg: 504.95 (1.0000) <0-02:55:02> 
Step:  132000, Reward: 672.441 [270.213], Avg: 506.209 (1.0000) <0-02:56:22> 
Step:  133000, Reward: 826.853 [77.856], Avg: 508.602 (1.0000) <0-02:57:43> 
Step:  134000, Reward: 722.054 [244.027], Avg: 510.183 (1.0000) <0-02:59:03> 
Step:  135000, Reward: 822.039 [118.517], Avg: 512.476 (1.0000) <0-03:00:23> 
Step:  136000, Reward: 841.546 [71.359], Avg: 514.878 (1.0000) <0-03:01:44> 
Step:  137000, Reward: 798.606 [199.486], Avg: 516.934 (1.0000) <0-03:03:03> 
Step:  138000, Reward: 838.608 [106.569], Avg: 519.248 (1.0000) <0-03:04:23> 
Step:  139000, Reward: 864.686 [50.955], Avg: 521.716 (1.0000) <0-03:05:44> 
Step:  140000, Reward: 790.532 [206.702], Avg: 523.622 (1.0000) <0-03:07:05> 
Step:  141000, Reward: 805.882 [140.198], Avg: 525.61 (1.0000) <0-03:08:28> 
Step:  142000, Reward: 832.112 [111.569], Avg: 527.753 (1.0000) <0-03:09:48> 
Step:  143000, Reward: 720.27 [217.316], Avg: 529.09 (1.0000) <0-03:11:08> 
Step:  144000, Reward: 780.558 [198.351], Avg: 530.824 (1.0000) <0-03:12:29> 
Step:  145000, Reward: 709.401 [199.237], Avg: 532.048 (1.0000) <0-03:13:50> 
Step:  146000, Reward: 847.882 [88.987], Avg: 534.196 (1.0000) <0-03:15:11> 
Step:  147000, Reward: 839.702 [55.551], Avg: 536.26 (1.0000) <0-03:16:32> 
Step:  148000, Reward: 858.126 [81.905], Avg: 538.421 (1.0000) <0-03:17:55> 
Step:  149000, Reward: 861.108 [80.532], Avg: 540.572 (1.0000) <0-03:19:17> 
Step:  150000, Reward: 886.707 [27.573], Avg: 542.864 (1.0000) <0-03:20:37> 
Step:  151000, Reward: 860.236 [62.082], Avg: 544.952 (1.0000) <0-03:21:59> 
Step:  152000, Reward: 868.928 [35.928], Avg: 547.07 (1.0000) <0-03:23:21> 
Step:  153000, Reward: 806.608 [155.195], Avg: 548.755 (1.0000) <0-03:24:43> 
Step:  154000, Reward: 861.88 [68.361], Avg: 550.775 (1.0000) <0-03:26:03> 
Step:  155000, Reward: 834.29 [113.647], Avg: 552.592 (1.0000) <0-03:27:24> 
Step:  156000, Reward: 838.438 [137.655], Avg: 554.413 (1.0000) <0-03:28:45> 
Step:  157000, Reward: 871.231 [49.705], Avg: 556.418 (1.0000) <0-03:30:06> 
Step:  158000, Reward: 807.115 [192.453], Avg: 557.995 (1.0000) <0-03:31:28> 
Step:  159000, Reward: 817.65 [150.254], Avg: 559.618 (1.0000) <0-03:32:48> 
Step:  160000, Reward: 822.887 [88.637], Avg: 561.253 (1.0000) <0-03:34:09> 
Step:  161000, Reward: 849.462 [123.268], Avg: 563.032 (1.0000) <0-03:35:26> 
