Model: <class 'models.ppo.PPOAgent'>, Dir: iter1/


import gym
import torch
import pickle
import argparse
import numpy as np
from models.rand import ReplayBuffer, PrioritizedReplayBuffer
from utils.network import PTACNetwork, PTACAgent, Conv, INPUT_LAYER, ACTOR_HIDDEN, CRITIC_HIDDEN, LEARN_RATE

EPS_MIN = 0.2                 	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.997             	# The rate at which eps decays from EPS_MAX to EPS_MIN
BATCH_SIZE = 5					# Number of samples to train on for each train step
PPO_EPOCHS = 4					# Number of iterations to sample batches for training
ENTROPY_WEIGHT = 0.005			# The weight for the entropy term of the Actor loss
CLIP_PARAM = 0.005				# The limit of the ratio of new action probabilities to old probabilities
NUM_STEPS = 100					# The number of steps to collect experience in sequence for each GAE calculation

class PPOActor(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)==1 else Conv(state_size, INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, ACTOR_HIDDEN)
		self.layer3 = torch.nn.Linear(ACTOR_HIDDEN, ACTOR_HIDDEN)
		self.action_mu = torch.nn.Linear(ACTOR_HIDDEN, *action_size)
		self.action_sig = torch.nn.Parameter(torch.zeros(*action_size))
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)
		
	def forward(self, state, action=None, sample=True):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		state = self.layer3(state).relu()
		action_mu = self.action_mu(state)
		action_sig = self.action_sig.exp().expand_as(action_mu)
		dist = torch.distributions.Normal(action_mu, action_sig)
		action = action_mu if not sample else dist.sample() if action is None else action
		log_prob = dist.log_prob(action)
		entropy = dist.entropy()
		return action, log_prob, entropy

class PPOCritic(torch.nn.Module):
	def __init__(self, state_size, action_size):
		super().__init__()
		self.layer1 = torch.nn.Linear(state_size[-1], INPUT_LAYER) if len(state_size)==1 else Conv(state_size, INPUT_LAYER)
		self.layer2 = torch.nn.Linear(INPUT_LAYER, CRITIC_HIDDEN)
		self.layer3 = torch.nn.Linear(CRITIC_HIDDEN, CRITIC_HIDDEN)
		self.value = torch.nn.Linear(CRITIC_HIDDEN, 1)
		self.apply(lambda m: torch.nn.init.xavier_normal_(m.weight) if type(m) in [torch.nn.Conv2d, torch.nn.Linear] else None)

	def forward(self, state):
		state = self.layer1(state).relu()
		state = self.layer2(state).relu()
		state = self.layer3(state).relu() + state
		value = self.value(state)
		return value

class PPONetwork(PTACNetwork):
	def __init__(self, state_size, action_size, lr=LEARN_RATE, gpu=True, load=None):
		super().__init__(state_size, action_size, PPOActor, PPOCritic, lr=lr, gpu=gpu, load=load)

	def get_action_probs(self, state, action_in=None, sample=True, grad=True):
		with torch.enable_grad() if grad else torch.no_grad():
			action, log_prob, entropy = self.actor_local(state.to(self.device), action_in, sample)
			return action if action_in is None else entropy.mean(), log_prob

	def get_value(self, state, grad=True):
		with torch.enable_grad() if grad else torch.no_grad():
			value = self.critic_local(state.to(self.device))
			return value

	def optimize(self, states, actions, old_log_probs, targets, advantages, importances=1, clip_param=CLIP_PARAM, e_weight=ENTROPY_WEIGHT, scale=1):
		values = self.get_value(states)
		critic_error = values - targets
		critic_loss = importances.to(self.device) + critic_error.pow(2)
		self.step(self.critic_optimizer, critic_loss.mean())

		entropy, new_log_probs = self.get_action_probs(states, actions)
		ratio = (new_log_probs - old_log_probs).exp()
		ratio_clipped = torch.clamp(ratio, 1.0-clip_param, 1.0+clip_param)
		actor_loss = -(torch.min(ratio*advantages, ratio_clipped*advantages).mean() + e_weight*entropy) * scale
		self.step(self.actor_optimizer, actor_loss)
		return critic_error.cpu().detach().numpy().squeeze(-1)

	def save_model(self, dirname="pytorch", name="best"):
		super().save_model("ppo", dirname, name)
		
	def load_model(self, dirname="pytorch", name="best"):
		super().load_model("ppo", dirname, name)

class PPOAgent(PTACAgent):
	def __init__(self, state_size, action_size, decay=EPS_DECAY, lr=LEARN_RATE, update_freq=NUM_STEPS, gpu=True, load=None):
		super().__init__(state_size, action_size, PPONetwork, lr=lr, update_freq=update_freq, decay=decay, gpu=gpu, load=load)
		self.replay_buffer = PrioritizedReplayBuffer()
		self.ppo_epochs = PPO_EPOCHS
		self.ppo_batch = BATCH_SIZE

	def get_action(self, state, eps=None, sample=True):
		state = self.to_tensor(state)
		self.action, self.log_prob = [x.cpu().numpy() for x in self.network.get_action_probs(state, sample=sample, grad=False)]
		return np.tanh(self.action)

	def train(self, state, action, next_state, reward, done):
		self.buffer.append((state, self.action, self.log_prob, reward, done))
		if len(self.buffer) >= int(self.update_freq * (1 - self.eps + EPS_MIN)**0.0):
			states, actions, log_probs, rewards, dones = map(self.to_tensor, zip(*self.buffer))
			self.buffer.clear()
			next_state = self.to_tensor(next_state)
			values = self.network.get_value(states, grad=False)
			next_value = self.network.get_value(next_state, grad=False)
			targets, advantages = self.compute_gae(next_value, rewards.unsqueeze(-1), dones.unsqueeze(-1), values)
			states, actions, log_probs, targets, advantages = [x.view(x.size(0)*x.size(1), *x.size()[2:]) for x in (states, actions, log_probs, targets, advantages)]
			self.replay_buffer.clear().extend(zip(states, actions, log_probs, targets, advantages))
			for _ in range(self.ppo_epochs*states.size(0)//self.ppo_batch):
				(states, actions, log_probs, targets, advantages), indices, importances = self.replay_buffer.sample(self.ppo_batch, dtype=torch.stack)
				errors = self.network.optimize(states, actions, log_probs, targets, advantages, importances**(1-self.eps), scale=20/self.update_freq)
				self.replay_buffer.update_priorities(indices, errors)
		if done[0]: self.eps = max(self.eps * self.decay, EPS_MIN)

REG_LAMBDA = 1e-6             	# Penalty multiplier to apply for the size of the network weights
LEARN_RATE = 0.0001           	# Sets how much we want to update the network weights at each training step
TARGET_UPDATE_RATE = 0.0004   	# How frequently we want to copy the local network to the target network (for double DQNs)
INPUT_LAYER = 512				# The number of output nodes from the first layer to Actor and Critic networks
ACTOR_HIDDEN = 256				# The number of nodes in the hidden layers of the Actor network
CRITIC_HIDDEN = 1024			# The number of nodes in the hidden layers of the Critic networks
DISCOUNT_RATE = 0.97			# The discount rate to use in the Bellman Equation
NUM_STEPS = 1000				# The number of steps to collect experience in sequence for each GAE calculation
EPS_MAX = 1.0                 	# The starting proportion of random to greedy actions to take
EPS_MIN = 0.1                 	# The lower limit proportion of random to greedy actions to take
EPS_DECAY = 0.995             	# The rate at which eps decays from EPS_MAX to EPS_MIN
ADVANTAGE_DECAY = 0.95			# The discount factor for the cumulative GAE calculation
MAX_BUFFER_SIZE = 100000      	# Sets the maximum length of the replay buffer

Ep: 0, Reward: -36.0424, Test: -25.0589 [13.98], Avg: -39.0384 (0.997)
Ep: 1, Reward: -36.3917, Test: -26.3597 [14.45], Avg: -39.9236 (0.994)
Ep: 2, Reward: -32.5893, Test: -23.8823 [14.09], Avg: -39.2723 (0.991)
Ep: 3, Reward: -35.1820, Test: -22.9615 [14.60], Avg: -38.8444 (0.988)
Ep: 4, Reward: -33.9006, Test: -24.3697 [10.92], Avg: -38.1330 (0.985)
Ep: 5, Reward: -30.8145, Test: -18.1985 [18.77], Avg: -37.9390 (0.982)
Ep: 6, Reward: -31.4317, Test: -24.3733 [15.16], Avg: -38.1668 (0.979)
Ep: 7, Reward: -30.2692, Test: -30.9614 [1.99], Avg: -37.5142 (0.976)
Ep: 8, Reward: -27.0822, Test: -20.8125 [13.33], Avg: -37.1399 (0.973)
Ep: 9, Reward: -30.3771, Test: -17.7008 [21.67], Avg: -37.3627 (0.970)
Ep: 10, Reward: -29.1620, Test: -25.6446 [9.34], Avg: -37.1464 (0.967)
Ep: 11, Reward: -30.6360, Test: -21.1617 [17.91], Avg: -37.3072 (0.965)
Ep: 12, Reward: -29.2158, Test: -21.9296 [17.89], Avg: -37.5006 (0.962)
Ep: 13, Reward: -26.2468, Test: -28.6732 [3.93], Avg: -37.1505 (0.959)
Ep: 14, Reward: -26.9247, Test: -21.1517 [17.18], Avg: -37.2289 (0.956)
Ep: 15, Reward: -28.5930, Test: -16.0151 [18.79], Avg: -37.0775 (0.953)
Ep: 16, Reward: -29.4233, Test: -13.9950 [15.17], Avg: -36.6123 (0.950)
Ep: 17, Reward: -27.2829, Test: -25.7939 [14.44], Avg: -36.8133 (0.947)
Ep: 18, Reward: -23.0426, Test: -16.7089 [15.87], Avg: -36.5906 (0.945)
Ep: 19, Reward: -29.3191, Test: -23.1813 [12.22], Avg: -36.5312 (0.942)
Ep: 20, Reward: -28.8044, Test: -8.5048 [17.60], Avg: -36.0346 (0.939)
Ep: 21, Reward: -29.5453, Test: -14.1338 [19.75], Avg: -35.9370 (0.936)
Ep: 22, Reward: -30.0211, Test: -23.1931 [16.59], Avg: -36.1043 (0.933)
Ep: 23, Reward: -26.7023, Test: -20.1143 [14.93], Avg: -36.0600 (0.930)
Ep: 24, Reward: -25.3869, Test: -16.9583 [17.37], Avg: -35.9906 (0.928)
Ep: 25, Reward: -25.6701, Test: -19.7571 [17.12], Avg: -36.0246 (0.925)
Ep: 26, Reward: -22.5100, Test: -26.9820 [17.53], Avg: -36.3389 (0.922)
Ep: 27, Reward: -30.5373, Test: -20.9262 [17.36], Avg: -36.4084 (0.919)
Ep: 28, Reward: -30.4884, Test: -28.9298 [4.99], Avg: -36.3225 (0.917)
Ep: 29, Reward: -24.0174, Test: -16.0064 [18.08], Avg: -36.2480 (0.914)
Ep: 30, Reward: -22.5614, Test: -12.0910 [17.37], Avg: -36.0292 (0.911)
Ep: 31, Reward: -25.5408, Test: -10.0352 [25.09], Avg: -36.0008 (0.908)
Ep: 32, Reward: -30.8215, Test: -24.1403 [11.14], Avg: -35.9791 (0.906)
Ep: 33, Reward: -29.1295, Test: -21.5170 [12.76], Avg: -35.9290 (0.903)
Ep: 34, Reward: -28.4750, Test: -17.7861 [17.22], Avg: -35.9025 (0.900)
Ep: 35, Reward: -27.7115, Test: -22.3447 [14.98], Avg: -35.9420 (0.897)
Ep: 36, Reward: -28.3583, Test: -13.8809 [23.73], Avg: -35.9871 (0.895)
Ep: 37, Reward: -26.2073, Test: -12.1339 [16.64], Avg: -35.7973 (0.892)
Ep: 38, Reward: -22.1331, Test: -31.0243 [6.57], Avg: -35.8434 (0.889)
Ep: 39, Reward: -22.9830, Test: -9.5427 [22.24], Avg: -35.7419 (0.887)
Ep: 40, Reward: -23.9816, Test: -11.8707 [19.36], Avg: -35.6320 (0.884)
Ep: 41, Reward: -25.7382, Test: -21.2931 [19.89], Avg: -35.7641 (0.881)
Ep: 42, Reward: -22.4255, Test: -28.1244 [10.22], Avg: -35.8241 (0.879)
